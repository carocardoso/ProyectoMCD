{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd35d588-ea62-4c69-bed5-9e4aa5df2371",
   "metadata": {
    "id": "fd35d588-ea62-4c69-bed5-9e4aa5df2371"
   },
   "source": [
    "# 3. Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5444ea-f091-438e-895e-8d1e3e2634ce",
   "metadata": {
    "id": "4f5444ea-f091-438e-895e-8d1e3e2634ce"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rdwjqqLYqcED",
   "metadata": {
    "id": "rdwjqqLYqcED",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zgVAAYIBdtMm",
   "metadata": {
    "id": "zgVAAYIBdtMm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a0741-7768-429f-9e17-4f1afa11bff3",
   "metadata": {
    "id": "546a0741-7768-429f-9e17-4f1afa11bff3"
   },
   "source": [
    "## 3.1 Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lV_3uJldd8UU",
   "metadata": {
    "id": "lV_3uJldd8UU"
   },
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV preparado en la etapa anterior en un DataFrame de pandas\n",
    "\n",
    "df = pd.read_csv('https://github.com/carocardoso/dataset_tesis/raw/refs/heads/main/datos_carr_sel_prepro.csv', sep=\";\", encoding='latin1')\n",
    "\n",
    "#ruta_csv = \"../datos/datos_carr_sel_prepro.csv\"\n",
    "#df = pd.read_csv(ruta_csv, encoding='latin1', sep=\";\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850e757-9406-42bf-a317-2301fbe7d006",
   "metadata": {
    "id": "f850e757-9406-42bf-a317-2301fbe7d006"
   },
   "source": [
    "## 3.2 Preparación de datos\n",
    "Esto se hizo en la etapa anterior pero se afina el preprocesamiento\n",
    "Se probó el modelado \"limpiando los textos\" y con el campo sin procesar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O8ST0U9a-WPL",
   "metadata": {
    "id": "O8ST0U9a-WPL"
   },
   "outputs": [],
   "source": [
    "# al generar el csv se agregó como salto de linea '_x000D_'. Se elimina aqui\n",
    "df['texto_limpio'] = df['texto_limpio'].str.replace('_x000D_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rFX1mJ3h54o3",
   "metadata": {
    "id": "rFX1mJ3h54o3"
   },
   "outputs": [],
   "source": [
    "# from langdetect import detect, LangDetectException\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize # Ensure word_tokenize is imported\n",
    "# from nltk.corpus import stopwords # Ensure stopwords are available if not global\n",
    "\n",
    "# def reemplazar_texto(texto):\n",
    "#   texto = texto.replace('_x000d_', '\\n')\n",
    "#   texto = texto.lower().replace('universidad católica de salta', 'UCASAL')\n",
    "#   # Remove 'dr.' as it might be a prefix and not meaningful\n",
    "#   texto = texto.lower().replace('dr.', 'doctor ')\n",
    "#   caracteres=['\\x93', '\\x94','\\x85','_x000D_', '_x d_','_x','d_']\n",
    "#   for caracter in caracteres:\n",
    "#     texto = texto.lower().replace(caracter, ' ')\n",
    "#   return texto\n",
    "\n",
    "# # def eliminar_texto_ingles(texto):\n",
    "# #   # Separar en párrafos\n",
    "# #   paragraphs = texto.split(\"\\n\")\n",
    "\n",
    "# #   # Filtrar solo párrafos en español\n",
    "# #   filtered_paragraphs = []\n",
    "# #   for p in paragraphs:\n",
    "# #       stripped_p = p.strip()\n",
    "# #       if len(stripped_p) > 0:\n",
    "# #           try:\n",
    "# #               if detect(stripped_p) == \"es\":\n",
    "# #                  filtered_paragraphs.append(p) # Append original p, not stripped_p, to preserve original formatting\n",
    "# #           except LangDetectException:\n",
    "# #               # Handle cases where langdetect cannot determine the language (e.g., \"000\")\n",
    "# #               pass # Skip paragraphs where language cannot be detected\n",
    "\n",
    "# #   # Reconstruir el texto en español\n",
    "# #   return \"\\n\".join(filtered_paragraphs)\n",
    "\n",
    "# def limpiar_texto(texto):\n",
    "#    # mantener puntuación y estructura\n",
    "#    texto = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ.,!?;:]', ' ', texto)\n",
    "\n",
    "#    # eliminar saltos de lineas y espacios vacios\n",
    "#    texto = re.sub(r'\\\\n', ' ', texto)\n",
    "#    texto = re.sub(r'\\s+', ' ', texto)\n",
    "\n",
    "#    # Convertir a minúsculas\n",
    "#    texto = texto.lower().strip()\n",
    "\n",
    "#    #eliminar números\n",
    "#    texto = re.sub(r'\\d+(.\\d+)?',' ', texto)\n",
    "\n",
    "#    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rfwuqfjekQif",
   "metadata": {
    "id": "rfwuqfjekQif"
   },
   "outputs": [],
   "source": [
    "# # completar los campos con nulos\n",
    "# df=df.fillna('')\n",
    "\n",
    "# # probar con todo el texto\n",
    "# df['texto_limpio'] = (df['titulo'] + '. ' + df['resumen']).apply(reemplazar_texto)\n",
    "# df['texto_limpio'] = df['texto_limpio'].apply(eliminar_texto_ingles)\n",
    "# df['texto_limpio'] = df['texto_limpio'].apply(limpiar_texto)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf53722-2a27-4f5a-a62a-bab7e274f52f",
   "metadata": {
    "id": "ccf53722-2a27-4f5a-a62a-bab7e274f52f"
   },
   "source": [
    "## 3.3 Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efd9ef-d87c-4f34-b5da-39cca51d9407",
   "metadata": {
    "id": "88efd9ef-d87c-4f34-b5da-39cca51d9407"
   },
   "outputs": [],
   "source": [
    "# agregar palabras consideradas vacías y que no están en stopwords\n",
    "words = ['así', 'ej','si','través']\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "#stopwords.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ksEtvAx3pPoY",
   "metadata": {
    "editable": true,
    "id": "ksEtvAx3pPoY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# opciones de configuración del modelo según cantidad de datos\n",
    "def config_model(cantidad_docs):\n",
    "  min_cluster_size = int(round(cantidad_docs * 0.05, 0)) + 1  # 5%  ## sumo 1 porque se puede generar tópico -1 (outliers)\n",
    "  n_neighbors = int(round(cantidad_docs * 0.08,0))  # 8%\n",
    "  outliers = int(round(cantidad_docs * 0.25,0))  # 25%\n",
    "  print(f'Cantidad de documentos: {cantidad_docs}.  min_cluster_size: {min_cluster_size}, n_neighbors: {n_neighbors}, outliers: {outliers}')\n",
    "\n",
    "  return min_cluster_size, n_neighbors, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0jXokofGkR2q",
   "metadata": {
    "id": "0jXokofGkR2q"
   },
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=stopwords, ngram_range=(1, 2))\n",
    "#embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# función para entrenar el modelo a partir de un conjunto de documentos\n",
    "def train_model(docs):\n",
    "  min_cluster_size, n_neighbors, outliers = config_model(len(docs))\n",
    "\n",
    " # hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean',  min_samples=1, prediction_data=True)\n",
    "  hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=10,        # ↑ Tamaño mínimo mayor para clusters más robustos\n",
    "    min_samples=3,             # ↑ Mayor valor para clusters más densos\n",
    "    cluster_selection_epsilon=0.05,  # ↓ Menor valor para evitar unir clusters cercanos\n",
    "    metric='euclidean'\n",
    ")\n",
    " # umap_model = UMAP(n_neighbors= n_neighbors, n_components=5, min_dist=0.0, metric='cosine', random_state=42) #n_components=por defecto es 5\n",
    "  umap_model = UMAP(\n",
    "    n_components=10,           # ↑ Mayor dimensionalidad para separar clusters\n",
    "    n_neighbors=15,            # ↑ Mayor valor para estructura más global\n",
    "    min_dist=0.2,              # ↑ Mayor distancia para separar clusters\n",
    "    metric='cosine',           # Usar cosine para datos textuales\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "  embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "  topic_model = BERTopic(\n",
    "  # Pipeline models\n",
    "  embedding_model=embedding_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  hdbscan_model = hdbscan_model,\n",
    "  umap_model=umap_model,\n",
    "\n",
    "  language=\"multilingual\",\n",
    "  #nr_topics= 6,\n",
    "  nr_topics=5,\n",
    "  # Hyperparameters\n",
    "    verbose=True\n",
    "  )\n",
    "\n",
    "  topics, probs = topic_model.fit_transform(docs, embeddings=embeddings)\n",
    "\n",
    "  # si hay muchos outliers, ajustar modelo\n",
    "  if (topics.count(-1) > outliers):\n",
    "    print(f\"  Reajustando modelo ...\")\n",
    "    # reducción de outliers\n",
    "    new_topics = topic_model.reduce_outliers(docs, topics, probabilities=probs, strategy=\"probabilities\")\n",
    "    #topic_model.update_topics(docs, topics=new_topics)\n",
    "    topic_model.update_topics(docs, vectorizer_model=vectorizer_model, embedding_model=embedding_model)\n",
    "\n",
    "  return topic_model #, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_XxR4DthkwlG",
   "metadata": {
    "id": "_XxR4DthkwlG"
   },
   "source": [
    "## 3.4 Obtener modelos para cada carrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K3Ssg2SGqbcv",
   "metadata": {
    "id": "K3Ssg2SGqbcv"
   },
   "outputs": [],
   "source": [
    "# Obtener lista de carreras seleccionadas\n",
    "carr_sel = df['carrera'].unique()\n",
    "carr_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Epi8RaTDCJfx",
   "metadata": {
    "id": "Epi8RaTDCJfx"
   },
   "outputs": [],
   "source": [
    "#guardar los modelos para evaluar y mostrar gráficos\n",
    "lista_modelos = pd.DataFrame(columns=['carrera', 'modelo'])\n",
    "\n",
    "topic_freq_list = []\n",
    "topic_docs_list = []\n",
    "\n",
    "for carr in carr_sel:\n",
    "    df_carr = df[df['carrera']==carr]\n",
    "\n",
    "    sel_docs = df_carr['texto_limpio'].tolist() # Convert Series to a list of strings\n",
    "    print('\\n'+carr+ '. Cantidad de docs: '+ str(len(sel_docs)))\n",
    "\n",
    "    # repetir para encontrar el mejor modelo que tenga entre 3 y 5 tópicos\n",
    "\n",
    "      # ENTRENAMIENTO con docs de la carrera seleccionada\n",
    "      topic_model = train_model(sel_docs)\n",
    "      if len(topic_model.get_topics()) > 5:\n",
    "\n",
    "    #tópicos frecuentes\n",
    "    freq = topic_model.get_topic_info()\n",
    "    freq['carrera'] = carr  # agregar una columna con la carrera\n",
    "    #freq['anio'] = df_carr['anio'] #.values  #df['anio']  #agregar columna año\n",
    "\n",
    "    # carrera, palabras de cada tópico y docs representativos\n",
    "    #nbres, desc = generar_titulo_topico(carr, topic_model.get_topic_info())\n",
    "    #freq['nombretopico'] = nbres\n",
    "    #freq['descripcion'] = desc\n",
    "\n",
    "    #agregar a la lista de tópicos frecuentes\n",
    "    topic_freq_list.append(freq) # Append DataFrame to the list\n",
    "\n",
    "    #asignar a cada documento el tópico que le corresponda, using the filtered documents\n",
    "    tdoc = topic_model.get_document_info(sel_docs)\n",
    "    tdoc['carrera'] = carr\n",
    "    tdoc['anio'] = df_carr['anio'] #.values  #df['anio']  #agregar columna año\n",
    "    topic_docs_list.append(tdoc)\n",
    "\n",
    "    # guardar el modelo (reducido) para usar en dashboard\n",
    "    nbre_modelo=\"model_\" + carr.replace(\" \",\"_\")\n",
    "\n",
    "    #topic_model.save(nbre_modelo)\n",
    "    topic_model.save(nbre_modelo, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "    lista_modelos = pd.concat([lista_modelos, pd.DataFrame({'carrera': [carr], 'modelo': nbre_modelo})])\n",
    "\n",
    "# Concatenate all DataFrames in the list after the loop\n",
    "topic_freq = pd.concat(topic_freq_list, ignore_index=True) if topic_freq_list else pd.DataFrame()\n",
    "topic_docs = pd.concat(topic_docs_list, ignore_index=True) if topic_docs_list else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QfPF9MkcqsxA",
   "metadata": {
    "id": "QfPF9MkcqsxA"
   },
   "outputs": [],
   "source": [
    "# #guardar los modelos para evaluar y mostrar gráficos\n",
    "# lista_modelos = pd.DataFrame(columns=['carrera', 'modelo'])\n",
    "\n",
    "# topic_freq_list = []\n",
    "# topic_docs_list = []\n",
    "\n",
    "# for carr in carr_sel:\n",
    "#     df_carr = df[df['carrera']==carr]\n",
    "\n",
    "#     sel_docs = df_carr['texto_limpio'].tolist() # Convert Series to a list of strings\n",
    "#     print('\\n'+carr+ '. Cantidad de docs: '+ str(len(sel_docs)))\n",
    "\n",
    "#     # ENTRENAMIENTO con docs de la carrera seleccionada\n",
    "#     topic_model = train_model(sel_docs)\n",
    "\n",
    "#     #tópicos frecuentes\n",
    "#     freq = topic_model.get_topic_info()\n",
    "#     freq['carrera'] = carr  # agregar una columna con la carrera\n",
    "#     #freq['anio'] = df_carr['anio'] #.values  #df['anio']  #agregar columna año\n",
    "\n",
    "#     # carrera, palabras de cada tópico y docs representativos\n",
    "#     #nbres, desc = generar_titulo_topico(carr, topic_model.get_topic_info())\n",
    "#     #freq['nombretopico'] = nbres\n",
    "#     #freq['descripcion'] = desc\n",
    "\n",
    "#     #agregar a la lista de tópicos frecuentes\n",
    "#     topic_freq_list.append(freq) # Append DataFrame to the list\n",
    "\n",
    "#     #asignar a cada documento el tópico que le corresponda, using the filtered documents\n",
    "#     tdoc = topic_model.get_document_info(sel_docs)\n",
    "#     tdoc['carrera'] = carr\n",
    "#     tdoc['anio'] = df_carr['anio'] #.values  #df['anio']  #agregar columna año\n",
    "#     topic_docs_list.append(tdoc)\n",
    "\n",
    "#     # guardar el modelo (reducido) para usar en dashboard\n",
    "#     nbre_modelo=\"model_\" + carr.replace(\" \",\"_\")\n",
    "\n",
    "#     #topic_model.save(nbre_modelo)\n",
    "#     topic_model.save(nbre_modelo, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "#     lista_modelos = pd.concat([lista_modelos, pd.DataFrame({'carrera': [carr], 'modelo': nbre_modelo})])\n",
    "\n",
    "# # Concatenate all DataFrames in the list after the loop\n",
    "# topic_freq = pd.concat(topic_freq_list, ignore_index=True) if topic_freq_list else pd.DataFrame()\n",
    "# topic_docs = pd.concat(topic_docs_list, ignore_index=True) if topic_docs_list else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CG2lvvztGhpc",
   "metadata": {
    "id": "CG2lvvztGhpc"
   },
   "outputs": [],
   "source": [
    "descarga=1  #descargar cada carpeta que contiene los archivos de modelos como un .zip\n",
    "if descarga==1:\n",
    "  # Code added to zip and download the model folders\n",
    "  from google.colab import files\n",
    "  import os\n",
    "\n",
    "  print(\"\\nDescargando modelos...\")\n",
    "  for index, row in lista_modelos.iterrows():\n",
    "      model_dir = row['modelo']\n",
    "      zip_filename = f\"{model_dir}.zip\"\n",
    "      if os.path.isdir(model_dir):\n",
    "          print(f\"Zipping {model_dir}...\")\n",
    "          !zip -r $zip_filename $model_dir\n",
    "          print(f\"Downloading {zip_filename}...\")\n",
    "          files.download(zip_filename)\n",
    "      else:\n",
    "          print(f\"Directory {model_dir} not found. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BE8LONFv5YNl",
   "metadata": {
    "id": "BE8LONFv5YNl"
   },
   "outputs": [],
   "source": [
    "topic_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iKwHC5RG6rcn",
   "metadata": {
    "id": "iKwHC5RG6rcn"
   },
   "outputs": [],
   "source": [
    "#ponerr todos los nombres de columnas en minúsculas para facilitar el proceso/visualización posterior\n",
    "topic_freq.columns = topic_freq.columns.str.lower()\n",
    "topic_docs.columns = topic_docs.columns.str.lower()\n",
    "\n",
    "# descargar los datos para mostrar\n",
    "topic_freq.to_csv('topic_freq.csv', index=False, encoding='latin1', sep=\";\" , quoting=1)\n",
    "topic_docs.to_csv('topic_docs.csv', index=False, encoding='latin1', sep=\";\" , quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asB6LOGUfSwj",
   "metadata": {
    "id": "asB6LOGUfSwj"
   },
   "source": [
    "## 3.5 Mostrar resultados de cada carrera seleccionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rYo3xal3lqCp",
   "metadata": {
    "id": "rYo3xal3lqCp"
   },
   "outputs": [],
   "source": [
    "def mostrar_resultados_por_carrera(carrera, df, model):\n",
    "  print(f\"Resultados del modelado de tópicos - {carrera}\")\n",
    "  print(\"-\"*(len(carrera)+40))\n",
    "\n",
    "  df_carrera = df[df['carrera']==carrera]\n",
    "\n",
    "    # Imprimo resulados\n",
    "  print(\"\\nTópicos\")\n",
    "  print(\"-----------\")\n",
    "  freq = model.get_topic_info()\n",
    "  display(freq)\n",
    "  print()\n",
    "\n",
    "  print(\"\\nTópicos y probabilidades\")\n",
    "  print(\"------------------------\")\n",
    "  # mostrar todos los tópicos cons las keys\n",
    "  df_topics = pd.DataFrame()   #auxiliar\n",
    "  all_topics = model.get_topics()\n",
    "  for topico in all_topics:\n",
    "    df_topic_words_temp = pd.DataFrame(model.get_topic(topico))\n",
    "    df_topic_words_temp.columns = [f'Topic{topico}_word', f'Topic{topico}_prob']\n",
    "    df_topics = pd.concat([df_topics, df_topic_words_temp], axis=1)\n",
    "\n",
    "  display(df_topics)\n",
    "\n",
    "  print('\\nRanking de palabras en cada tópico')\n",
    "  print('----------------------------------')\n",
    " # barchart de ranking de palabras por tópico (se inclyen 5 keys por tópico por claridad)\n",
    "  fig1 = model.visualize_barchart(top_n_topics=len(model.get_topics()), n_words=5)\n",
    "  fig1.show()\n",
    "\n",
    "  if (len(all_topics)-1) > 2:\n",
    "    print('\\nDistribución de clusters')\n",
    "    print('------------------------')\n",
    "    # interactivo distribución/distancia entre tópicos\n",
    "    try:\n",
    "      fig2 = model.visualize_topics()\n",
    "      fig2.show()\n",
    "    except TypeError as e:\n",
    "      print(f\"  Error al visualizar la distribución de clusters (visualize_topics): {e}\")\n",
    "      print(\"  Esto puede ocurrir con un número muy pequeño de tópicos. Se omitirá esta visualización.\")\n",
    "\n",
    "  print('\\nJerarquía de clusters')\n",
    "  print('---------------------')\n",
    "\n",
    "  # jerarquia de clusters\n",
    "  fig3 = model.visualize_hierarchy(top_n_topics=50) # Commented out due to ValueError with few topics\n",
    "  fig3.show()\n",
    "\n",
    "  print('\\nEvolución de tópicos por años')\n",
    "  print('---------------------------')\n",
    "  # Evolución de tópicos por años\n",
    "  timestamps = df_carrera['anio'].tolist()\n",
    "  topics_over_time = model.topics_over_time(df_carrera['texto_limpio'], timestamps, nr_bins=20)\n",
    "  fig4 =  model.visualize_topics_over_time(topics_over_time, top_n_topics=10)\n",
    "  fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ILx_YMHIoM7J",
   "metadata": {
    "id": "ILx_YMHIoM7J"
   },
   "outputs": [],
   "source": [
    "#cargo cada modelo para ver los resultados\n",
    "\n",
    "elto_lista_mod = lista_modelos.iloc[0]\n",
    "model_name = elto_lista_mod['modelo']\n",
    "carrera =elto_lista_mod['carrera']\n",
    "\n",
    "df_datos = df[df['carrera'] == elto_lista_mod['carrera']]\n",
    "\n",
    "# cargar el modelo\n",
    "modelo = BERTopic.load(model_name, embedding_model)\n",
    "\n",
    "# mostrar resultados en forma visual\n",
    "mostrar_resultados_por_carrera(carrera, df_datos, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AqGgoy9e0qyr",
   "metadata": {
    "id": "AqGgoy9e0qyr"
   },
   "outputs": [],
   "source": [
    "elto_lista_mod = lista_modelos.iloc[1]\n",
    "model_name = elto_lista_mod['modelo']\n",
    "carrera =elto_lista_mod['carrera']\n",
    "\n",
    "df_datos = df[df['carrera'] == elto_lista_mod['carrera']]\n",
    "\n",
    "# cargar el modelo\n",
    "modelo = BERTopic.load(model_name,embedding_model)\n",
    "\n",
    "# mostrar resultados en forma visual\n",
    "mostrar_resultados_por_carrera(carrera, df_datos, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183781f4-33f0-4784-9df8-67ab861853ce",
   "metadata": {
    "id": "183781f4-33f0-4784-9df8-67ab861853ce"
   },
   "outputs": [],
   "source": [
    "elto_lista_mod = lista_modelos.iloc[2]\n",
    "model_name = elto_lista_mod['modelo']\n",
    "carrera =elto_lista_mod['carrera']\n",
    "\n",
    "df_datos = df[df['carrera'] == elto_lista_mod['carrera']]\n",
    "\n",
    "# cargar el modelo\n",
    "modelo = BERTopic.load(model_name, embedding_model)\n",
    "\n",
    "# mostrar resultados en forma visual\n",
    "mostrar_resultados_por_carrera(carrera, df_datos, modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be782b99-3b0d-430b-9eec-3fce1b68af34",
   "metadata": {
    "id": "be782b99-3b0d-430b-9eec-3fce1b68af34"
   },
   "source": [
    "## Attributes\n",
    "\n",
    "There are a number of attributes that you can access after having trained your BERTopic model:\n",
    "\n",
    "\n",
    "| Attribute | Description |\n",
    "|------------------------|---------------------------------------------------------------------------------------------|\n",
    "| topics_               | The topics that are generated for each document after training or updating the topic model. |\n",
    "| probabilities_ | The probabilities that are generated for each document if HDBSCAN is used. |\n",
    "| topic_sizes_           | The size of each topic                                                                      |\n",
    "| topic_mapper_          | A class for tracking topics and their mappings anytime they are merged/reduced.             |\n",
    "| topic_representations_ | The top *n* terms per topic and their respective c-TF-IDF values.                             |\n",
    "| c_tf_idf_              | The topic-term matrix as calculated through c-TF-IDF.                                       |\n",
    "| topic_labels_          | The default labels for each topic.                                                          |\n",
    "| custom_labels_         | Custom labels for each topic as generated through `.set_topic_labels`.                                                               |\n",
    "| topic_embeddings_      | The embeddings for each topic if `embedding_model` was used.                                                              |\n",
    "| representative_docs_   | The representative documents for each topic if HDBSCAN is used.                                                |\n",
    "\n",
    "For example, to access the predicted topics for the first 10 documents, we simply run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C_PSslygKKwq",
   "metadata": {
    "id": "C_PSslygKKwq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T7jv9GgxSJng",
   "metadata": {
    "id": "T7jv9GgxSJng"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9344e5-b5ac-478d-ae5a-2fde117fd021",
   "metadata": {
    "id": "3a9344e5-b5ac-478d-ae5a-2fde117fd021"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04825012-313a-47ab-aa72-c21ce91f5528",
   "metadata": {
    "id": "04825012-313a-47ab-aa72-c21ce91f5528"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(top_n_topics=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E3UlmAjDQCvo",
   "metadata": {
    "id": "E3UlmAjDQCvo"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sy2vSgxdQhWn",
   "metadata": {
    "id": "Sy2vSgxdQhWn"
   },
   "outputs": [],
   "source": [
    "#topic_model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NXCxVAjsQygP",
   "metadata": {
    "id": "NXCxVAjsQygP"
   },
   "outputs": [],
   "source": [
    "timestamps = df_carr['anio'].tolist()\n",
    "\n",
    "topics_over_time = topic_model.topics_over_time(df_carr['texto_limpio'], timestamps, nr_bins=20)\n",
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26GLrSblZTM",
   "metadata": {
    "id": "f26GLrSblZTM"
   },
   "outputs": [],
   "source": [
    "labels = topic_model.generate_topic_labels(nr_words=5)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QN90bkRb7xI5",
   "metadata": {
    "id": "QN90bkRb7xI5"
   },
   "source": [
    "ver esto: https://colab.research.google.com/#fileId=https%3A//huggingface.co/spaces/davanstrien/blog_notebooks/blob/main/BERTopic_hub_starter.ipynb\n",
    "\n",
    "topic over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iWWstJFd24Mq",
   "metadata": {
    "id": "iWWstJFd24Mq"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df =pd.DataFrame()\n",
    "# # Ejemplo de dos DataFrames\n",
    "# df1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']})\n",
    "# df2 = pd.DataFrame({'C': ['C0', 'C1'], 'D': ['D0', 'D1']})\n",
    "\n",
    "# df_concatenado = pd.concat([df, df1], axis=1)\n",
    "\n",
    "# print(df_concatenado)\n",
    "# # Concatenar por columnas\n",
    "# df_concatenado = pd.concat([df_concatenado, df2], axis=1)\n",
    "\n",
    "# print(df_concatenado)\n",
    "\n",
    "\n",
    "# df_concatenado = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# print(df_concatenado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc065c8e",
   "metadata": {
    "id": "cc065c8e"
   },
   "source": [
    "# Task\n",
    "Modify the `mostrar_resultados_por_carrera` function in cell `rYo3xal3lqCp` to collect all `df_topic_words` (each with columns named 'TopicX_word' and 'TopicX_prob') into a single `df_topics` DataFrame by concatenating them column-wise, and then display the final `df_topics` DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173289d",
   "metadata": {
    "id": "4173289d"
   },
   "source": [
    "## consolidate_topic_words_into_dataframe\n",
    "\n",
    "### Subtask:\n",
    "Modify the `mostrar_resultados_por_carrera` function in cell `rYo3xal3lqCp` to collect all `df_topic_words` (each with columns named 'TopicX_word' and 'TopicX_prob') into a single `df_topics` DataFrame by concatenating them column-wise, and then display the final `df_topics` DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a814a",
   "metadata": {
    "id": "7d4a814a"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "*   The `mostrar_resultados_por_carrera` function was successfully modified to collect all individual topic word DataFrames (`df_topic_words`) into a single `df_topics` DataFrame.\n",
    "*   This consolidation was achieved by concatenating the individual topic word DataFrames column-wise, resulting in a comprehensive DataFrame containing 'TopicX_word' and 'TopicX_prob' columns for all topics.\n",
    "\n",
    "### Insights or Next Steps\n",
    "*   The consolidated `df_topics` DataFrame provides a unified view of the most relevant words and their probabilities across all identified topics, which can be used for a holistic understanding of the thematic structure.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
