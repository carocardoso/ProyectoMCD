{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f2bd26-d349-453a-8218-095a1011db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d626e835-23f6-4a36-93bb-d249290f7551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\accar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e55b70-c223-482f-86c7-5a1a5a1ff4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671bcd61-2d96-4001-90d8-030ad71346da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DESCARGAR PDF SI NO EXISTE\n",
    "# ============================================================\n",
    "#def descargar_pdf(url, carpeta=\"pdfs\"):\n",
    "def descargar_pdf(url, carpeta):\n",
    "    os.makedirs(carpeta, exist_ok=True)\n",
    "\n",
    "    nombre = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "    if not nombre.lower().endswith(\".pdf\"):\n",
    "        nombre += \".pdf\"\n",
    "\n",
    "    path_pdf = os.path.join(carpeta, nombre)\n",
    "\n",
    "    if os.path.exists(path_pdf):\n",
    "        return path_pdf\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        with open(path_pdf, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return path_pdf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# EXTRAER TEXTO DE CADA PÁGINA DEL PDF\n",
    "# ============================================================\n",
    "def extraer_paginas_pdf(path_pdf):\n",
    "    try:\n",
    "        reader = PdfReader(path_pdf)\n",
    "        return [(page.extract_text() or \"\") for page in reader.pages]\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo PDF {path_pdf}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893cdb4b-9e62-495c-93db-64a9a268eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRAER SECCIÓN \"INTRODUCCIÓN\" EVITANDO ÍNDICE Y PORTADA\n",
    "# ============================================================\n",
    "\n",
    "# Regex robusta para detectar encabezado de sección\n",
    "PATRON_INICIO = r\"\"\"\n",
    "    (?:^|\\n)                                # inicio de línea\n",
    "    (?:\\d+\\s*\\.?\\s*|[IVX]+\\.\\s*)*           # opcional: numeraciones\n",
    "    (Introducci[oó]n)                       # palabra clave\n",
    "    \\s*\\n                                   # salto de línea = encabezado real\n",
    "\"\"\"\n",
    "\n",
    "# Regex genérica que detecta el siguiente título\n",
    "PATRON_FIN = r\"\"\"\n",
    "    (?:^|\\n)\n",
    "    (?:\\d+\\s*\\.?\\s*|[IVX]+\\.\\s*)+           # algo como \"2.\" o \"III.\"\n",
    "    [A-ZÁÉÍÓÚÑ][^\\n]{2,80}\\n                # título siguiente\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extraer_introduccion_regex_paginas(paginas, min_pagina=2, max_pagina=12):\n",
    "    \"\"\"\n",
    "    páginas: lista de textos por página\n",
    "    min_pagina: evita portada e índice\n",
    "    max_pagina: evita procesar PDFs enormes\n",
    "    \"\"\"\n",
    "\n",
    "    if len(paginas) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    # Limitamos el rango útil\n",
    "    paginas_utiles = paginas[min_pagina:max_pagina]\n",
    "    texto = \"\\n\".join(paginas_utiles)\n",
    "\n",
    "    # Buscar encabezado real\n",
    "    inicio = re.search(PATRON_INICIO, texto, flags=re.IGNORECASE | re.VERBOSE)\n",
    "    if not inicio:\n",
    "        return \"\"\n",
    "\n",
    "    inicio_idx = inicio.start()\n",
    "\n",
    "    # Buscar fin\n",
    "    fin = re.search(PATRON_FIN, texto[inicio_idx:], flags=re.IGNORECASE | re.VERBOSE)\n",
    "    if fin:\n",
    "        fin_idx = inicio_idx + fin.start()\n",
    "    else:\n",
    "        fin_idx = len(texto)\n",
    "\n",
    "    intro = texto[inicio_idx:fin_idx].strip()\n",
    "\n",
    "    # Limpieza opcional con NLTK (mejora la coherencia)\n",
    "    try:\n",
    "        oraciones = sent_tokenize(intro, language=\"spanish\")\n",
    "        intro_limpia = \" \".join(oraciones)\n",
    "        return intro_limpia\n",
    "    except:\n",
    "        return intro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16856640-47ef-41ef-9fe9-263752939d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE COMPLETO: PDF + EXTRACCIÓN + VALIDACIÓN SEMÁNTICA\n",
    "# ============================================================\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd # Import pandas here for explicit use\n",
    "\n",
    "def descargar_pdf(url, path_destino):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=20)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print(f\"   → No se pudo descargar {url}: HTTP {r.status_code}\")\n",
    "            return False\n",
    "\n",
    "        with open(path_destino, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "        # Validar encabezado PDF\n",
    "        with open(path_destino, \"rb\") as f:\n",
    "            if f.read(5) != b\"%PDF-\":\n",
    "                print(\"   → Archivo descargado NO es PDF real (probable HTML).\")\n",
    "                os.remove(path_destino)\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def procesar_archivo(csv_entrada, csv_salida, carpeta_pdf):\n",
    "    df = pd.read_csv(csv_entrada, encoding='latin1', sep=\";\")\n",
    "\n",
    "    if \"pdf\" not in df.columns:\n",
    "        raise ValueError(\"El CSV debe contener una columna llamada 'pdf'.\")\n",
    "\n",
    "    # Create a copy to work on and avoid SettingWithCopyWarning\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Initialize the 'introduccion' column for all rows\n",
    "    df_processed[\"introduccion\"] = \"\"\n",
    "\n",
    "    # Crear carpeta si no existe\n",
    "    os.makedirs(carpeta_pdf, exist_ok=True)\n",
    "\n",
    "    for i, url in df_processed[\"pdf\"].items():\n",
    "        print(f\"\\nProcesando fila {i}:\")\n",
    "        print(f\"URL original: {url}\")\n",
    "        print(f\"Tipo de URL: {type(url)}\")\n",
    "\n",
    "        # Robustly check for NaN or empty string. pd.isna handles float('nan').\n",
    "        if pd.isna(url) or (isinstance(url, str) and not url.strip()):\n",
    "            print(\"   → URL no válida (NaN o vacía). Saltando.\\n\")\n",
    "            df_processed.loc[i, \"introduccion\"] = \"No existe pdf\"\n",
    "            continue  # Skip to the next iteration\n",
    "\n",
    "        # Ensure URL is a string and clean it up\n",
    "        url = str(url).strip()\n",
    "\n",
    "        # Nombre del archivo PDF\n",
    "        nombre_pdf_file = url.split(\"/\")[-1].split(\"?\")[0].strip()\n",
    "        if not nombre_pdf_file.lower().endswith(\".pdf\"):\n",
    "            nombre_pdf_file += \".pdf\"\n",
    "        \n",
    "        path_pdf = os.path.join(carpeta_pdf, nombre_pdf_file)\n",
    "\n",
    "        # If the PDF doesn't exist locally, download it\n",
    "        if not os.path.exists(path_pdf):\n",
    "            print(f\"   → Descargando {url} a {path_pdf}...\")\n",
    "            exito = descargar_pdf(url, path_pdf)\n",
    "        else:\n",
    "            print(f\"   → Archivo ya existe: {path_pdf}\")\n",
    "            exito = True  # It already exists\n",
    "\n",
    "        if exito:\n",
    "            df_processed.loc[i, \"introduccion\"] = \"descargado\"\n",
    "            #Original commented out code for extraction and semantic validation\n",
    "            try:\n",
    "                # Extraer texto por páginas\n",
    "                paginas = extraer_paginas_pdf(path_pdf)\n",
    "\n",
    "                # Extraer sección introducción\n",
    "                intro = extraer_introduccion_regex_paginas(paginas)\n",
    "\n",
    "                # Validación semántica con embeddings\n",
    "                if es_introduccion_valida(intro):\n",
    "                    df_processed.loc[i, \"introduccion\"] = intro\n",
    "                else:\n",
    "                    print(\"   → Introducción NO válida.\")\n",
    "                    df_processed.loc[i, \"introduccion\"] = \"Intro no válids\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error leyendo PDF:\", e)\n",
    "                df_processed.loc[i, \"introduccion\"] = \"No extrae Intro\"\n",
    "        else:\n",
    "            print(f\"   → NO se pudo descargar el PDF de {url}\")\n",
    "            df_processed.loc[i, \"introduccion\"] = \"No descarga pdf\"\n",
    "\n",
    "    #df_processed.to_csv(csv_salida, index=False, encoding='latin1', sep=';')\n",
    "    #print(f\"\\n>>> Archivo generado: {csv_salida}\")\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3959938-f9e3-4e70-a915-d315d88327da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EJECUTAR PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "entrada=\"../datos/datos_carr_sel.csv\"\n",
    "salida = \"../datos/datos_pdfs.csv\"\n",
    "carpeta_pdf =\"../pdfs\"\n",
    "df_salida = procesar_archivo(entrada, salida, carpeta_pdf)\n",
    "df_salida.to_csv(csv_salida, index=False, encoding='latin1', sep=';')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
